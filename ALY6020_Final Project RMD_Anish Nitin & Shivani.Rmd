---
title: "ALY6020 Final Project : Prediction of Molecular Properties"
author: Shivani Grover, Anish Nitin Somaiah
date: "June 28, 2019"
output: html_document
---


#INTRODUCTION:

[1]There are two distinct types of magnetic interaction (coupling) between nuclei (A and X) with a non-zero spin - the direct interaction (dipole-dipole coupling: D) and the indirect or scalar coupling (spin-spin splitting: J). The direct interaction is about 1000 times as large as the scalar coupling (e.g. at 2 Å distance H-H dipolar coupling is ca 30,000 Hz). These direct couplings make the observation of high-resolution NMR spectra in solids and very viscous liquids difficult, and make NMR spectra in liquid crystals (where molecules are partially oriented, and the dipolar coupling is only partially averaged) very complex.

The scalar coupling J is a through-bond interaction, in which the spin of one nucleus perturbs (polarizes) the spins of the intervening electrons, and the energy levels of neighboring magnetic nuclei are in turn perturbed by the polarized electrons. This leads to a lowering of the energy of the neighboring nucleus when the perturbing nucleus has one spin, and a raising of the energy when it has the other spin. The J coupling (always reported in Hz) is field-independent

Coupling constants can be either positive or negative, defined as follows: coupling constants are positive if the energy of A is lower when X has the opposite spin as A, and negative if the energy of A is lower when X has the same spin as A. 



#PROJECT OBJECTIVE AND METHODOLOGY
The objective of our project is to predict the molecular property i.e. interaction between atoms in a molecule (scalar coupling constant) for every molecule in the test dataset by training a regression machine learning model over a given training data set.

Steps involved in the project include:
1.	Data Preparation.

2.	Correlation analysis to identify dependencies for the target variable.

3.	Exploratory data analytics for dependent variables.

4.	Modelling using regression techniques and evaluating accuracy of models.

5.	Final prediction using the optimal model.



#DATA SET DESCRIPTION
The dataset is obtained from one of the active Kaggle competitions " Predicting Molecular Properties" hosted by CHAMPS (CHemistry And Mathematics in Phase Space) retrieved from https://www.kaggle.com/c/champs-scalar-coupling/data .

The following spreadsheets were used in our model and are as follows:

1.	train.csv: this is the training data file with close to 4 million rows comprising of the molecule name, atom indices that form the pair, joint type of atoms in the molecule and their respective scalar coupling constants.
2. test.csv: this is the test file similar to that of the training file with different molecule names for which the scalar coupling constant is to be predicted.
3.	structures.csv: this is the file that gives positional data of different atoms in a particular molecule by its x,y and z coordinates.
4.	mulliken_charges.csv: this is the file that gives the respective mulliken charge of each atom in a molecule.
5.	potential_energy.csv: this is the file that gives the potential energy of each molecule.
6.	dipole_moments.csv: this is the file that gives X,Y and Z components of the dipole moments for each molecule.


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,eval = TRUE)
```

#IMPORTING LIBRARIES AND DATA FILES
```{r echo=TRUE, eval=TRUE}
library('dplyr')
library('xgboost')
library('tidyverse')
library('Metrics')
library('ggplot2')
library('gridExtra')
library('grid')
library('corrplot')


train <- read.csv("C:/Users/anish/Downloads/Predictive Analytics/Final project/champs-scalar-coupling/train.csv") 
test <- read.csv("C:/Users/anish/Downloads/Predictive Analytics/Final project/champs-scalar-coupling/test.csv" )
dipole <- read.csv("C:/Users/anish/Downloads/Predictive Analytics/Final project/champs-scalar-coupling/dipole_moments.csv")
structure <- read.csv("C:/Users/anish/Downloads/Predictive Analytics/Final project/champs-scalar-coupling/structures.csv")
potential <- read.csv("C:/Users/anish/Downloads/Predictive Analytics/Final project/champs-scalar-coupling/potential_energy.csv")
muliken <- read.csv("C:/Users/anish/Downloads/Predictive Analytics/Final project/champs-scalar-coupling/mulliken_charges.csv")
```


#TRAINING DATA FRAME PROPERTIES
```{r echo=TRUE, eval=TRUE}
summary(train)
str(train)
```


# DATA PREPARATION
Theoretically, the scalar coupling constant depends upon the distance between atoms in the molecule. 
But given dataset has only coordinates of atoms and no distance values. 
Hence distances are first calculated using the given x,y and z coordinates by formula 
 

```{r echo=TRUE, eval=TRUE}
sc <- train$scalar_coupling_constant
train$scalar_coupling_constant <- NULL

full <- rbind(train,test) %>% 
  left_join(structure, by = c("molecule_name","atom_index_0" = "atom_index")) %>% 
  left_join(structure, by = c("molecule_name","atom_index_1" = "atom_index")) %>%
  mutate(
    x_dist = x.x - x.y, 
    y_dist = y.x - y.y,  
    z_dist = z.x - z.y, 
    dist = sqrt(x_dist^2 + y_dist^2 + z_dist^2)) 
names(full) <- c("id", "molecule_name","atom_index_0", "atom_index_1", "joint_type","atom_0","x_0", "y_0", "z_0", "atom_1",
                  "x_1","y_1","z_1", "x_dist","y_dist", "z_dist","distance")

train_f <- full[1:nrow(train),c("molecule_name","atom_index_0","atom_index_1","joint_type","distance")]
train_f$scalar_coupling_constant <- as.vector(sc)
```

#EDA 1 : FILES 
```{r echo=TRUE, eval=TRUE}
summary(train_f)
summary(test)
summary(structure)

```

#EDA 2 SCALAR COUPLING CONSTANT  
The Frequency distribution histogram of scalar coupling constant indicates that the distribution of coupling constant in training data set is bimodal which also proves the low accuracy of the model in the beginning.

#EDA 3 ORIENTATION DISTRIBUTION
The distribution of orientation of atomic spin in a molecule has a huge difference between the same spin and opposite spin molecules and this bias can affect the accuracy of our model. Ideally distribution should be equal in the training dataset to get a model with higher accuracy.


```{r echo=TRUE, eval=TRUE}
#Histogram
ggplot(train_f, aes(x = scalar_coupling_constant)) +
  geom_histogram(color = "black", fill = "orange") + ggtitle("Frequency Histogram of Scalar Coupling Constant")

#Bar Plot
s1 <- nrow(subset(train_f,train_f$scalar_coupling_constant<0))
s2 <- nrow(subset(train_f,train_f$scalar_coupling_constant>0))
df <- data.frame(Orientation=c("Opposite spin","Same spin"),
                 Molecule_count=c(s2,s1))
ggplot(data=df, aes(x=Orientation, y=Molecule_count,fill=Orientation)) +
  geom_bar(stat="identity") + ggtitle("Atomic Orientation Distribution") 
```


#EDA 4 : CORRELATION PLOT
By theory and as per obtained correlation plot we observe that coupling constants depend upon distance between atoms in a molecule and hence distance becomes the most significant dependent variable for the target variable in our model.

The other variables such as mulliken charge, potential energy, dipole moments have either 0 or low correlation and hence are not considered as dependencies for the target variable in the model.

```{r echo=TRUE, eval=TRUE}
train1 <- full[1:nrow(train),c("molecule_name","atom_index_0","atom_index_1","joint_type",
                                "distance")]
train1$scalar_coupling_constant <- as.numeric(sc)
train2 <- train1[, c("atom_index_0", "atom_index_1","distance", "scalar_coupling_constant")]

dipole$diople_moment <- sqrt(dipole$X^2 + dipole$Y^2 + dipole$Z^2) 
tr_dip <- train1 %>%
  left_join(dipole, by = c("molecule_name"))%>%
  left_join(potential, by = "molecule_name")%>%
  left_join(muliken, by = c("molecule_name", "atom_index_0" = "atom_index"))%>%
  left_join(muliken, by = c("molecule_name", "atom_index_1" = "atom_index"))
names(tr_dip) <-c ("molecule_name","atom_index_0","atom_index_1", "joint_type","distance", 
                      "coupling_const", "X", "Y", "Z", "dipole", 
                      "potential", "mulliken0", "mulliken1")
c <- tr_dip[, c("atom_index_0", "atom_index_1", "distance", "coupling_const",
                "dipole", "potential", "mulliken0", "mulliken1")]

d <- na.omit(c) 
M <- cor(d)
corrplot.mixed(M, lower.col ="black", number.cex=0.9, upper = "circle")
```

#EDA 5 : Scalar Coupling Constant Distribution by Atomic Type

The distribution of coupling constant for different joint types are either right or left skewed with minor peaks in the skew direction. 

1JHC : 

.	This is a molecule made by a single bond between a hydrogen and carbon atom. 
.	There are 709416 molecules of this combination in the data set which is distributed in the data set with right skew. 
.	The molecules of this atomic type have positive coupling constants only i.e. molecules of this atomic type have atoms spinning only in opposite directions

2JHC : 

.	This is a molecule made by a double bond between a hydrogen and carbon atom. 
.	There are 1140674 molecules of this combination in the data set which is symmetrically distributed. 
.	This atomic type molecules have equal no.of positive and negative coupling constants i.e. molecules of this atomic type have atoms spinning in both opposite and parallel directions

3JHC :

.	This is a molecule made by a triple bond between a hydrogen and carbon atom. 
.	There are 1510379 molecules of this combination in the data set which is the highest and normally distributed in the data set with right skew.
.	The molecules of this atomic type have positive coupling constants only i.e. molecules of this atomic type have atoms spinning only in opposite directions and the difference from 1JHC is that the magnitude of coupling constant is very low in triple bond compared to single bond.

2JHH : 

.	This is a molecule made by a double bond between a two hydrogen atoms. 
.	There are 378036 molecules of this combination in the data set which is symmetrically distributed with peak in negative scale.
.	The molecules of this atomic type have a majority of negative coupling constants  i.e. a majority of the molecules of this atomic type have atoms spinning in parallel directions

3JHH : 

.	This is a molecule made by a triple bond between a two hydrogen atoms.
.	There are 590611 molecules of this combination in the data set which is distributed with many peaks skewing towards the right. 
.	The molecules of this atomic type have a majority of positive coupling constants  i.e. a majority of the molecules of this atomic type have atoms spinning in opposite directions.

3JHN : 

.	This is a molecule made by a triple bond between a two hydrogen atoms.
.	There are 166415 molecules of this combination in the data set which is distributed with many peaks skewing towards the right. 
.	The molecules of this atomic type have positive coupling constants only i.e. molecules of this atomic type have atoms spinning only in opposite directions and the difference from 3JHC is that the magnitude of coupling constant is very low in triple bond with nitrogen than bond with carbon.


```{r echo=TRUE, eval=TRUE}
p1 <- ggplot(filter(train_f, scalar_coupling_constant > 0,joint_type == "1JHC"), aes(x = scalar_coupling_constant)) +
      geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "skyblue",
            linetype = "solid") +
      labs(x="Type = 1JHC")

p2 <- ggplot(filter(train_f,scalar_coupling_constant > 0, joint_type == "2JHC"), aes(x = scalar_coupling_constant)) +
  geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "lightgreen",
            linetype = "solid") +
  labs(x="Type = 2JHC")


p3 <- ggplot(filter(train_f,scalar_coupling_constant > 0, joint_type == "3JHC"), aes(x = scalar_coupling_constant)) +
  geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "orange",
            linetype = "solid") +
  labs(x="Type = 3JHC")

p4 <- ggplot(filter(train_f, scalar_coupling_constant > 0,joint_type == "2JHH"), aes(x = scalar_coupling_constant)) +
  geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "red",
            linetype = "solid") +
  labs(x="Type = 2JHH")


p5 <- ggplot(filter(train_f,scalar_coupling_constant > 0, joint_type == "3JHH"), aes(x = scalar_coupling_constant)) +
  geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "yellow",
            linetype = "solid") +
  labs(x="Type = 3JHH")


p6 <- ggplot(filter(train_f,scalar_coupling_constant > 0, joint_type == "3JHN"), aes(x = scalar_coupling_constant)) +
  geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "purple",
            linetype = "solid") +
  labs(x="Type = 3JHN")

grid.arrange(p1,p2,p3,p4,p5,p6, ncol=3,
             top = textGrob("Distribution of Coupling Constant by Type for Opposite Spin",gp=gpar(fontsize=15)))

p7 <- ggplot(filter(train_f, scalar_coupling_constant < 0,joint_type == "1JHC"), aes(x = scalar_coupling_constant)) +
      geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "skyblue",
            linetype = "solid") +
      labs(x="Type = 1JHC")

p8 <- ggplot(filter(train_f,scalar_coupling_constant < 0, joint_type == "2JHC"), aes(x = scalar_coupling_constant)) +
  geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "lightgreen",
            linetype = "solid") +
  labs(x="Type = 2JHC")


p9 <- ggplot(filter(train_f,scalar_coupling_constant < 0, joint_type == "3JHC"), aes(x = scalar_coupling_constant)) +
  geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "orange",
            linetype = "solid") +
  labs(x="Type = 3JHC")

p10 <- ggplot(filter(train_f, scalar_coupling_constant < 0,joint_type == "2JHH"), aes(x = scalar_coupling_constant)) +
  geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "red",
            linetype = "solid") +
  labs(x="Type = 2JHH")


p11 <- ggplot(filter(train_f,scalar_coupling_constant < 0, joint_type == "3JHH"), aes(x = scalar_coupling_constant)) +
  geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "yellow",
            linetype = "solid") +
  labs(x="Type = 3JHH")


p12 <- ggplot(filter(train_f,scalar_coupling_constant < 0, joint_type == "3JHN"), aes(x = scalar_coupling_constant)) +
  geom_area(stat = "bin", 
            binwidth = 0.5, 
            colour = "black",
            fill = "purple",
            linetype = "solid") +
  labs(x="Type = 3JHN")

grid.arrange(p7,p8,p9,p10,p11,p12, ncol=3,
             top = textGrob("Distribution of Coupling Constant by Type for Parallel Spin",gp=gpar(fontsize=15)))

```

#MODELLING THE DATA:

From the above correlation plot and EDA of coupling constant for different types, we infer that for the target variable i.e. scalar coupling constant, the significant dependent variables are atomic distance and joint type of the atoms in the molecule.
Based on these 2 dependencies we implement two models in our training dataset, evaluate the accuracy parameters of the models with the predicted values using the training dataset and use the models to predict scalar coupling constant in the test dataframe. 


#Model 1: Linear Regression 

Linear Regression is the methood used to determine the values of dependent variable(scalar coupling constant) using the independent variables(distance and joint type). It indicates that the reponse variable is linearly dependent on the independent variable in the form of an equation: y = mx + c where, c is the intercept value and m is the cofficient of the independent variable x and y being the dependent variable.
the dependency of x and y can be positive/negative, that is, if an incement of x leads to increase inthe value of y hten they are positively correlated and vice-versa.

From EDA of the response variable, scalar coupling constant, there are sufficient evidences for its dependence on atomic distance which is in turn affected by the type of joint between the atoms. Using the 'distance' parameter calculated in data preparation and the 'joint type' variable, generated a linear regression model to determine the dependency of scalar coupling constant on these variables.  Using lm() function is R, we get a p-value of 2e-16, verifying that the independent factors strongly affect the J - interaction between the atoms. The obtained estimated coefficients are used to get the model equation and compute the values for new instances of the test set. We get an adjusted R-square value of 94%, which further convinces us about the reliability of the model.  The mean absolute error 



```{r echo=TRUE, eval=TRUE}
reg <- lm(scalar_coupling_constant~ distance + joint_type , data = train_f)
summary(reg)
train_f$pred <- predict(reg, newdata = train_f)
test1 <- full[(nrow(train)+1):nrow(full),]
test1$predlm <- predict(reg, newdata = test1)
prediction_lm <- test1$predlm
```

#Model 2: Gradient Boosting 
Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of and ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.This algorithm has base model is based on creation of decision trees. IN this method initially a tree model is produced and predicitons are made using it. The error calculated using those predicted values is used to adjust the predictions to train the next models using lazy learning technique. Lazy learning is an ensemble of weakly trained (adjusted) models which eventually lead to a strong model genration which produces least possible errors while predicting the response variable for the test set. 

The objective parameter used in gradient boosting is linear regression with evaluation parameter being mae (mean absolute error). The number of iterations, that is, nrounds is 100 and the max depth of the tree is kept 0.2. the learning rate, which defines the proportion of last model output to be included in the next model, is kept at 0.5 after many test values such as, 0.01, 0.1, 0.3, and finally 0.5. In this model, a regression tree is being trained repeated using lazy learning technique. 

```{r echo=TRUE, eval=TRUE}
train1 <- xgb.DMatrix(data = as.matrix(train_f[, c("distance")]), label = train_f$scalar_coupling_constant)
test2 <- xgb.DMatrix(data = as.matrix(test1[, c("distance")]))
xgb_params <- list("objective" = "reg:linear",
                   "eval_metric" = "mae")

model <- xgb.train(params = xgb_params,
                   data = train1,
                   eta = 0.5,
                   nrounds = 100,
                   max_depth = 2,
                   subsample = 0.9,
                   colsample_bytree = 1)

train_pred_xgb <- predict(model,  newdata = train1)
prediction_xgb <- predict(model, newdata = test2)


```

#Error Calculations

The mean absolute error observed in the linear regression model is 4.134
The mean absolute error obtained in gradient boosting model is 4.221
Thus, we observed that the mean absolute error is increasing in the case of gradient boosting

```{r echo=TRUE, eval=TRUE}
mae <- function(error)
{
  mean(abs(error))
}
mae(train_f$pred-train_f$scalar_coupling_constant)
mae(train_pred_xgb-train_f$scalar_coupling_constant)

```


#Final Predictions

After Accuracy evaluation we have used both the models to predict scalar coupling constant in the test datset and combined both model predictions into a single dataframe.

```{r echo=TRUE, eval=TRUE}

final <- test1$molecule_name
final <- cbind(final,prediction_lm)
final <- cbind(final,prediction_xgb)
head(final)
```

##Conclusions

Altering the 'xgb' parameters, like objective, nrounds, eta, max depth, and adding new parameters like min_child_weight, base_score, best_score, feature_names would help to improve accuracy by reducing the mean absolute error values.

Adding the effect of other variables such as mulliken charges, dipole moments, and potential energy will also, improve the accuracy but we may run into the possibility of creating an overfitted model. Moreover, adding interaction variables with the given variables to train the model will also reduce the mean absolute error values of the entire model. This is beacuse the variables are not completely independent of each other. 

Since, the error value in the performed xgb inceases, making linear model to be more accurate with the given parameters.



#REFERENCE
[1]Hans J. Reich (2017), Spin-Spin Splitting: J-Coupling, Retrieved from https://www.chem.wisc.edu/areas/reich/nmr/Notes-05-HMR-v26-part2.pdf

